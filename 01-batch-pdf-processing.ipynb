{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch PDF processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "In this hourlong session, we will walk through **Python** code that extracts data from **nine PDFs**.\n",
    "\n",
    "### What if I need to parse bigger batches of PDFs, like 1,000 or 10,000 or 1,000,000 or... ?\n",
    "\n",
    "- Check out [how ICIJ mined 715,000 records](https://www.icij.org/investigations/luanda-leaks/how-we-mined-more-than-715000-luanda-leaks-records/) with their open-source tool [Datashare](https://www.icij.org/blog/2019/11/what-is-datashare-frequently-asked-questions-about-our-document-analysis-software/)\n",
    "\n",
    "- Check out Miguel Barbosa's 2018 [presentation](https://docs.google.com/presentation/d/12ShGfr3HhSXdLwK5R1pr1J5cvjLXuEmaXGxErG3gShg/preview)\n",
    "\n",
    "- Check out Jacob Fenton's [session](https://ireapps.github.io/nicar-2020-schedule#20200308_pdf_3_batch_pdf_processing_repeat_2478) on Sunday\n",
    "\n",
    "### Concepts apply no matter how many PDFs you have\n",
    "\n",
    "- Know the types of PDFs\n",
    "- Trial and error\n",
    "- Divide and conquer\n",
    "- Check the facts\n",
    "- [Collaborate!](https://source.opennews.org/articles/news-nerds-against-pdfs/)\n",
    "\n",
    "\n",
    "### The challenge\n",
    "\n",
    "ICE has a [page](https://www.ice.gov/facility-inspections) of detention facility inspections in PDF. Most inspections involve a \"cover letter\" that lists violations standards and their components and a \"summary review form\" that tabulates incidents and provides narratives.\n",
    "\n",
    "For a USA TODAY Network [investigation](https://www.usatoday.com/in-depth/news/nation/2019/12/19/ice-detention-private-prisons-expands-under-trump-administration/4393366002/) of private prisons that detain immigrants, we wanted to count violations from these reports.\n",
    "\n",
    "In this notebook, I'll demonstrate parsing grievances filed from summary forms with [pdfplumber](https://github.com/jsvine/pdfplumber).\n",
    "\n",
    "### Why [pdfplumber](https://github.com/jsvine/pdfplumber)?\n",
    "\n",
    "There are many open-source tools that can work with PDFs! The [2019 Q4 IRE Journal](https://www.ire.org/wp-content/uploads/2019/12/IREJournal_Q4_2019.pdf) had a useful list of tools \n",
    "\n",
    "I turned to pdfplumber for a few reasons: I'm familiar with python, I had previous experience with pdfplumber and because pdfplumber was built by journalist Jeremy Singer-Vine and [contributors](https://github.com/jsvine/pdfplumber#acknowledgments--contributors).\n",
    "\n",
    "Most importantly, I was confident pdfplumber could work for the task because the PDFs were of the \"native\" [type](https://docs.google.com/presentation/d/12ShGfr3HhSXdLwK5R1pr1J5cvjLXuEmaXGxErG3gShg/preview?slide=id.g1ebfd012eb_0_40)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the directory\n",
    "!ls pdf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(\n",
    "    'pdf/jenaLaSalle_SIS_09-26-2019.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many pages does this PDF have?\n",
    "\n",
    "len(pdf.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first page\n",
    "\n",
    "page = pdf.pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the first page look like?\n",
    "\n",
    "page.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are the grievances in this PDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pdf.pages[5]\n",
    "page.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the grievances in a different PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(\n",
    "    'pdf/allenParishDetFac_SIS_02-14-2019.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pdf.pages[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Oh snap!_ What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf.pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect metadata about our PDFs\n",
    "\n",
    "We have just learned that different PDFs have different numbers of pages. This is a great chance to learn more about our documents by creating metadata about them.\n",
    "\n",
    "How many pages does each PDF have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a python library that helps us find batches of files\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob('pdf/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display each PDF and the number of pages it has\n",
    "\n",
    "for file_name in glob('pdf/*'):\n",
    "    pdf = pdfplumber.open(file_name)\n",
    "    print(\n",
    "        file_name,\n",
    "        len(pdf.pages)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a python library that helps us work with data\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of dictionaries that contain metadata\n",
    "\n",
    "payload = []\n",
    "\n",
    "for file_name in glob('pdf/*'):\n",
    "    pdf = pdfplumber.open(file_name)\n",
    "    payload.append({\n",
    "        'file_name': file_name,\n",
    "        'pages': len(pdf.pages)\n",
    "    })\n",
    "\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe of the metadata and display the dataframe\n",
    "\n",
    "pdf_meta = pd.DataFrame(\n",
    "    payload\n",
    ")\n",
    "\n",
    "pdf_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know these records are different page lengths, but where are the grievance tables in each PDF?\n",
    "\n",
    "### Find patterns in text with regular expressions\n",
    "\n",
    "Let's practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find pages that contain \"Grievances:\" `OR` \"grievances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_grievance_table = r'REGEX_HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Even just learning how to write a simple loop is very helpful to convert PDFs.\" - Todd Wallack, [NICAR 2017](https://www.ire.org/resource-center/audio/1223/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = []\n",
    "\n",
    "# loop over all the PDF files in the directory\n",
    "\n",
    "for file_name in glob('pdf/*'):\n",
    "    pdf = pdfplumber.open(file_name)\n",
    "    \n",
    "    # loop over each page in the opened PDF\n",
    "    \n",
    "    for page_index, page in enumerate(pdf.pages):\n",
    "\n",
    "        if re.search(\n",
    "                pattern_grievance_table,\n",
    "                page.extract_text()\n",
    "            ):\n",
    "            \n",
    "            # add metadata to the list if the grievance pattern is found\n",
    "\n",
    "            payload.append({\n",
    "                'file_name': file_name,\n",
    "                'table_page': page_index\n",
    "            })\n",
    "            \n",
    "            \n",
    "pdf_grievance_pages = pd.DataFrame(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_meta = pdf_grievance_pages.merge(\n",
    "    pdf_meta,\n",
    "    on='file_name'\n",
    ")\n",
    "\n",
    "pdf_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know where the grievance data are on each document. Time to parse!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse a single PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata for a PDF\n",
    "\n",
    "pdf_row = pdf_meta.loc[\n",
    "    lambda x: x['file_name'] == 'pdf/allenParishDetFac_SIS_02-14-2019.pdf'\n",
    "].iloc[0]\n",
    "\n",
    "pdf_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(pdf_row['file_name'])\n",
    "\n",
    "# use our metadata to open the page with the grievance data and view it\n",
    "\n",
    "page = pdf.pages[\n",
    "    pdf_row['table_page']\n",
    "]\n",
    "\n",
    "page.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pdfplumber tablefinder!\n",
    "\n",
    "page.to_image().debug_tablefinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The debugger looks messy because the PDF has inconsistent rows.\n",
    "\n",
    "We could parse the whole page and clean it up as a second step, but we can also zoom in to a smaller area and extract it with more accuracy.\n",
    "\n",
    "In this case, we just want the \"Greivances Received\" row. We can `crop` the page using pixel coordinates.\n",
    "\n",
    "pdfplumber can crop pdfs with bounding boxes, which are `\"4-tuple with the values (x0, top, x1, bottom)\"`\n",
    "\n",
    "Another way to remember it: `(left, top, right, bottom)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_coordinates = (0, 300, page.width, page.height)\n",
    "\n",
    "page_cropped = page.crop(crop_coordinates)\n",
    "\n",
    "page_cropped.to_image().debug_tablefinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the coordinates exactly right can be tricky.\n",
    "\n",
    "I opened a PDF in Adobe Illustrator and used its interface to find coordinates, but we can also use pdfplumber to find coordinates.\n",
    "\n",
    "pdfplumber provides the coordinates of `words`, we can use those coordinates to determine where we should crop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.extract_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_grievances = list(filter(\n",
    "    lambda x: x['text'] == 'Grievances:',\n",
    "    page.extract_words()\n",
    "))[0]\n",
    "\n",
    "coordinate_grievances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial and error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_left = coordinate_grievances['x1'] + 100\n",
    "coordinate_right = page.width\n",
    "coordinate_top = coordinate_grievances['top'] - 5\n",
    "coordinate_bottom = coordinate_grievances['top'] + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_coordinates = (coordinate_left, coordinate_top, coordinate_right, coordinate_bottom)\n",
    "\n",
    "crop_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.within_bbox(\n",
    "    crop_coordinates\n",
    ").to_image().debug_tablefinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, let's get the table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_table = page.within_bbox(\n",
    "    crop_coordinates\n",
    ").extract_table()\n",
    "\n",
    "extracted_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are just one row, but let's make a pandas DataFrame out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    extracted_table,\n",
    "    columns=['q1', 'q2', 'q3', 'q4']\n",
    ").astype(int).assign(\n",
    "    total = lambda x: x.sum(axis=1),\n",
    "    file=file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract grievances from a nine-page PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_row = pdf_meta.loc[\n",
    "    lambda x: x['pages'] == 9\n",
    "].iloc[0]\n",
    "\n",
    "pdf = pdfplumber.open(pdf_row['file_name'])\n",
    "\n",
    "page = pdf.pages[\n",
    "    pdf_row['table_page']\n",
    "]\n",
    "\n",
    "page.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_grievances = list(filter(\n",
    "    lambda x: x['text'] == 'grievances',\n",
    "    page.extract_words()\n",
    "))[0]\n",
    "\n",
    "# I found these exact values after trial and error!\n",
    "coordinate_left = coordinate_grievances['x1']\n",
    "coordinate_right = page.width\n",
    "coordinate_top = coordinate_grievances['top'] - 5\n",
    "coordinate_bottom = coordinate_grievances['top'] + 30\n",
    "\n",
    "crop_coordinates = (coordinate_left, coordinate_top, coordinate_right, coordinate_bottom)\n",
    "\n",
    "page.within_bbox(\n",
    "    crop_coordinates\n",
    ").to_image(resolution=150).debug_tablefinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crop looks close, but notice tablefinder is adding more lines than we need. We can use one of the many pdfplumber [table settings](https://github.com/jsvine/pdfplumber#table-extraction-settings) to fine tune the table. In this case, I found the `snap_tolerance` setting helped with the multiple lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    page.within_bbox(\n",
    "        crop_coordinates\n",
    "    ).extract_table({\n",
    "        'snap_tolerance': 8\n",
    "    }),\n",
    "    columns = [\n",
    "        'ice',\n",
    "        'non_ice',\n",
    "        'total'\n",
    "    ]\n",
    ").astype(int).assign(\n",
    "    file=file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've extracted data from a 4-page PDF and a 9-page PDF, and we can apply these techniques to our entire batch.\n",
    "\n",
    "\"Even just learning how to write a simple loop is very helpful to convert PDFs.\" - Todd Wallack, [NICAR 2017](https://www.ire.org/resource-center/audio/1223/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    pdf = pdfplumber.open(row['file_name'])\n",
    "    page = pdf.pages[row['table_page']]\n",
    "    page_count = row['pages']\n",
    "    \n",
    "    coordinate_grievances = list(filter(\n",
    "        lambda x: re.search(\n",
    "            pattern_grievance_table, x['text']\n",
    "        ),\n",
    "        page.extract_words()\n",
    "    ))[0]\n",
    "    \n",
    "    coordinate_left = coordinate_grievances['x1']\n",
    "    coordinate_right = page.width\n",
    "    \n",
    "    if row['pages'] == 9:\n",
    "        coordinate_top = coordinate_grievances['top'] - 5\n",
    "        coordinate_bottom = coordinate_grievances['top'] + 30\n",
    "        \n",
    "    else:\n",
    "        coordinate_left = coordinate_left + 100\n",
    "        coordinate_top = coordinate_grievances['top'] - 5\n",
    "        coordinate_bottom = coordinate_grievances['top'] + 30\n",
    "\n",
    "    crop_coordinates = (coordinate_left, coordinate_top, coordinate_right, coordinate_bottom)\n",
    "        \n",
    "    page = page.within_bbox(crop_coordinates)\n",
    "    \n",
    "    if page_count == 9:\n",
    "        return pd.DataFrame(\n",
    "            page.extract_table({\n",
    "                'snap_tolerance': 8\n",
    "            }),\n",
    "            columns = [\n",
    "                'ice',\n",
    "                'non_ice',\n",
    "                'total'\n",
    "            ]\n",
    "        ).astype(int).assign(\n",
    "            total=lambda x: x['ice'] + x['non_ice'],\n",
    "            file=row['file_name']\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return pd.DataFrame(\n",
    "            page.extract_table(),\n",
    "            columns=[\n",
    "                'q1',\n",
    "                'q2',\n",
    "                'q3',\n",
    "                'q4'\n",
    "            ]\n",
    "        ).astype(int).assign(\n",
    "            total=lambda x: x.sum(axis=1),\n",
    "            file=row['file_name']\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function is ready, let's run it on all the PDFs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = []\n",
    "\n",
    "for index, row in pdf_meta.iterrows():\n",
    "    payload.append(process_row(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where is `'N/A'` coming from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfplumber.open(\n",
    "    pdf_meta.iloc[1]['file_name']\n",
    ").pages[\n",
    "    pdf_meta.iloc[1]['table_page']\n",
    "].to_image(resolution=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function needs a tweak to convert `'N/A'` to `0` so we can add up grievances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    pdf = pdfplumber.open(row['file_name'])\n",
    "    page = pdf.pages[row['table_page']]\n",
    "    page_count = row['pages']\n",
    "    \n",
    "    coordinate_grievances = list(filter(\n",
    "        lambda x: re.search(\n",
    "            pattern_grievance_table, x['text']\n",
    "        ),\n",
    "        page.extract_words()\n",
    "    ))[0]\n",
    "    \n",
    "    coordinate_left = coordinate_grievances['x1']\n",
    "    coordinate_right = page.width\n",
    "    \n",
    "    if row['pages'] == 9:\n",
    "        coordinate_top = coordinate_grievances['top'] - 5\n",
    "        coordinate_bottom = coordinate_grievances['top'] + 30\n",
    "        \n",
    "    else:\n",
    "        coordinate_left = coordinate_left + 100\n",
    "        coordinate_top = coordinate_grievances['top'] - 5\n",
    "        coordinate_bottom = coordinate_grievances['top'] + 30\n",
    "\n",
    "    crop_coordinates = (coordinate_left, coordinate_top, coordinate_right, coordinate_bottom)\n",
    "        \n",
    "        \n",
    "    page = page.within_bbox(crop_coordinates)\n",
    "    \n",
    "    if page_count == 9:\n",
    "        return pd.DataFrame(\n",
    "            page.extract_table({\n",
    "                'snap_tolerance': 8\n",
    "            }),\n",
    "            columns = [\n",
    "                'ice',\n",
    "                'non_ice',\n",
    "                'total'\n",
    "            ]\n",
    "        ).astype(int).assign(\n",
    "            total=lambda x: x['ice'] + x['non_ice'],\n",
    "            file=row['file_name']\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return pd.DataFrame(\n",
    "            page.extract_table(),\n",
    "            columns=[\n",
    "                'q1',\n",
    "                'q2',\n",
    "                'q3',\n",
    "                'q4'\n",
    "            ]\n",
    "        ).replace('N/A', 0).astype(int).assign(\n",
    "            total=lambda x: x.sum(axis=1),\n",
    "            file=row['file_name']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = []\n",
    "\n",
    "for index, row in pdf_meta.iterrows():\n",
    "    payload.append(process_row(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(payload)[['file', 'total']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at all the PDFs we parsed!\n",
    "\n",
    "But processing actually invovled quite a few things. We:\n",
    "\n",
    "- created pandas data frames about our data\n",
    "- wrote `for` loops\n",
    "- used regular expressions\n",
    "- filtered lists\n",
    "- used trial-and-error to find the perfect PDF coordinates\n",
    "- debugged errors\n",
    "\n",
    "pdfplumber helped collect data from the reports, but we also used [Overview](https://blog.overviewdocs.com/) to search text and OCR non-machine-readable documents we encountered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
