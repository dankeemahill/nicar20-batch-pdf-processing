{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch PDF parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "ICE has a [page](https://www.ice.gov/facility-inspections) of PDFs describing detention facility inspections. Most inspections involve a \"cover letter\" that lists violations standards and their components and a \"summary review form\" that tabulates incidents and provides narratives.\n",
    "\n",
    "There are 132 inspections between July 2018 and November 2019, and we'd like to parse some statistics about violations and incidents from those reports.\n",
    "\n",
    "In this notebook, I'll demonstrate parsing grievances filed from summary forms with pdfplumber.\n",
    "\n",
    "## The Plan\n",
    "\n",
    "1. Study the PDFs\n",
    "    1. Single PDF\n",
    "        1. which page has the table?\n",
    "    2. All the PDFs\n",
    "        1. create a meta table\n",
    "2. Extract data\n",
    "    1. Single PDF\n",
    "    2. All the PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the directory\n",
    "!ls pdf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(\n",
    "    'pdf/jenaLaSalle_SIS_09-26-2019.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pdf.pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the grievances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pdf.pages[5]\n",
    "page.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check a different PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(\n",
    "    'pdf/allenParishDetFac_SIS_02-14-2019.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pdf.pages[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf.pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect metadata about our PDFs\n",
    "\n",
    "How many pages does each PDF have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob('pdf/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in glob('pdf/*'):\n",
    "    pdf = pdfplumber.open(file_name)\n",
    "    print(\n",
    "        file_name,\n",
    "        len(pdf.pages)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = []\n",
    "\n",
    "for file_name in glob('pdf/*'):\n",
    "    pdf = pdfplumber.open(file_name)\n",
    "    payload.append({\n",
    "        'file_name': file_name,\n",
    "        'pages': len(pdf.pages)\n",
    "    })\n",
    "\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_meta = pd.DataFrame(\n",
    "    payload\n",
    ")\n",
    "\n",
    "pdf_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know these records are different page lengths, but where are the grievance tables?\n",
    "\n",
    "Regular expression can help find patterns in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_grievance_table = r'Grievances:|grievances'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = []\n",
    "\n",
    "\n",
    "for file_name in glob('pdf/*'):\n",
    "    pdf = pdfplumber.open(file_name)\n",
    "    for page_index, page in enumerate(pdf.pages):\n",
    "        if re.search(\n",
    "                pattern_grievance_table,\n",
    "                page.extract_text()\n",
    "            ):\n",
    "            payload.append({\n",
    "                'file_name': file_name,\n",
    "                'table_page': page_index\n",
    "            })\n",
    "            \n",
    "pdf_meta = pd.DataFrame(payload).merge(\n",
    "    pdf_meta,\n",
    "    on='file_name'\n",
    ")\n",
    "\n",
    "pdf_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know where the grievance data are on each document. Time to parse!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single PDF processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_row = pdf_meta.loc[\n",
    "    lambda x: x['file_name'] == 'pdf/allenParishDetFac_SIS_02-14-2019.pdf'\n",
    "].iloc[0]\n",
    "\n",
    "pdf_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(pdf_row['file_name'])\n",
    "\n",
    "page = pdf.pages[\n",
    "    pdf_row['table_page']\n",
    "]\n",
    "\n",
    "page.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.to_image().debug_tablefinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The debugger looks messy because the PDF has inconsistent rows.\n",
    "\n",
    "We could parse the whole page and clean it up as a second step, but we can also zoom in to a smaller area and extract it with more accuracy.\n",
    "\n",
    "In this case, we just want the \"Greivances Received\" row. We can `crop` the page using pixel coordinates.\n",
    "\n",
    "pdfplumber can crop pdfs with bounding boxes, which are `\"4-tuple with the values (x0, top, x1, bottom)\"`\n",
    "\n",
    "Another way to remember it: `(left, top, right, bottom)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_coordinates = (0, 300, page.width, page.height)\n",
    "\n",
    "page_cropped = page.crop(crop_coordinates)\n",
    "\n",
    "page_cropped.to_image().debug_tablefinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the coordinates just right can be tricky.\n",
    "\n",
    "I opened a PDF in Adobe Illustrator and used its interface to find coordinates, but we can also programatically find coordinates.\n",
    "\n",
    "pdfplumber provides the coordinates of `words`, which we can use for our crop coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.extract_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_grievances = list(filter(\n",
    "    lambda x: x['text'] == 'Grievances:',\n",
    "    page.extract_words()\n",
    "))[0]\n",
    "\n",
    "coordinate_grievances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial and error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_left = coordinate_grievances['x1'] + 100\n",
    "coordinate_right = page.width\n",
    "coordinate_top = coordinate_grievances['top'] - 5\n",
    "coordinate_bottom = coordinate_grievances['top'] + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_coordinates = (coordinate_left, coordinate_top, coordinate_right, coordinate_bottom)\n",
    "\n",
    "crop_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.within_bbox(\n",
    "    crop_coordinates\n",
    ").to_image().debug_tablefinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much beter, let's get the table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_table = page.within_bbox(\n",
    "    crop_coordinates\n",
    ").extract_table()\n",
    "\n",
    "extracted_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are just one row, but let's make a pandas DataFrame out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    extracted_table,\n",
    "    columns=['q1', 'q2', 'q3', 'q4']\n",
    ").astype(int).assign(\n",
    "    total = lambda x: x.sum(axis=1),\n",
    "    file=file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract grievances from a nine-page PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_row = pdf_meta.loc[\n",
    "    lambda x: x['pages'] == 9\n",
    "].iloc[0]\n",
    "\n",
    "pdf = pdfplumber.open(pdf_row['file_name'])\n",
    "\n",
    "page = pdf.pages[\n",
    "    pdf_row['table_page']\n",
    "]\n",
    "\n",
    "page.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_grievances = list(filter(\n",
    "    lambda x: x['text'] == 'grievances',\n",
    "    page.extract_words()\n",
    "))[0]\n",
    "\n",
    "# I found these values after trial and error!\n",
    "coordinate_left = coordinate_grievances['x1']\n",
    "coordinate_right = page.width\n",
    "coordinate_top = coordinate_grievances['top'] - 5\n",
    "coordinate_bottom = coordinate_grievances['top'] + 30\n",
    "\n",
    "crop_coordinates = (coordinate_left, coordinate_top, coordinate_right, coordinate_bottom)\n",
    "\n",
    "page.within_bbox(\n",
    "    crop_coordinates\n",
    ").to_image(resolution=150).debug_tablefinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crop looks close, but notice tablefinder is adding more lines than we need. We can use one of the many pdfplumber [table settings](https://github.com/jsvine/pdfplumber#table-extraction-settings) to fine tune the table. In this case, I found the `snap_tolerance` setting helped with the multiple lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    page.within_bbox(\n",
    "        crop_coordinates\n",
    "    ).extract_table({\n",
    "        'snap_tolerance': 8\n",
    "    }),\n",
    "    columns = [\n",
    "        'ice',\n",
    "        'non_ice',\n",
    "        'total'\n",
    "    ]\n",
    ").astype(int).assign(\n",
    "    file=file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've extracted data from a 4-page PDF and a 9-page PDF, and we can apply these techniques to our entire batch.\n",
    "\n",
    "> \"Even just learning how to write a simple loop is very helpful to convert PDFs.\" - Todd Wallack, [NICAR 2017](https://www.ire.org/resource-center/audio/1223/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    pdf = pdfplumber.open(row['file_name'])\n",
    "    page = pdf.pages[row['table_page']]\n",
    "    page_count = row['pages']\n",
    "    \n",
    "    coordinate_grievances = list(filter(\n",
    "        lambda x: re.search(\n",
    "            pattern_grievance_table, x['text']\n",
    "        ),\n",
    "        page.extract_words()\n",
    "    ))[0]\n",
    "    \n",
    "    coordinate_left = coordinate_grievances['x1']\n",
    "    coordinate_right = page.width\n",
    "    \n",
    "    if row['pages'] == 9:\n",
    "        coordinate_top = coordinate_grievances['top'] - 5\n",
    "        coordinate_bottom = coordinate_grievances['top'] + 30\n",
    "        \n",
    "    else:\n",
    "        coordinate_left = coordinate_left + 100\n",
    "        coordinate_top = coordinate_grievances['top'] - 5\n",
    "        coordinate_bottom = coordinate_grievances['top'] + 30\n",
    "\n",
    "    crop_coordinates = (coordinate_left, coordinate_top, coordinate_right, coordinate_bottom)\n",
    "        \n",
    "        \n",
    "    page = page.within_bbox(crop_coordinates)\n",
    "    \n",
    "    if page_count == 9:\n",
    "        return pd.DataFrame(\n",
    "            page.extract_table({\n",
    "                'snap_tolerance': 8\n",
    "            }),\n",
    "            columns = [\n",
    "                'ice',\n",
    "                'non_ice',\n",
    "                'total'\n",
    "            ]\n",
    "        ).astype(int).assign(\n",
    "            total=lambda x: x['ice'] + x['non_ice'],\n",
    "            file=row['file_name']\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return pd.DataFrame(\n",
    "            page.extract_table(),\n",
    "            columns=[\n",
    "                'q1',\n",
    "                'q2',\n",
    "                'q3',\n",
    "                'q4'\n",
    "            ]\n",
    "        ).astype(int).assign(\n",
    "            total=lambda x: x.sum(axis=1),\n",
    "            file=row['file_name']\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = []\n",
    "\n",
    "for index, row in pdf_meta.iterrows():\n",
    "    payload.append(process_row(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    pdf = pdfplumber.open(row['file_name'])\n",
    "    page = pdf.pages[row['table_page']]\n",
    "    page_count = row['pages']\n",
    "    \n",
    "    coordinate_grievances = list(filter(\n",
    "        lambda x: re.search(\n",
    "            pattern_grievance_table, x['text']\n",
    "        ),\n",
    "        page.extract_words()\n",
    "    ))[0]\n",
    "    \n",
    "    coordinate_left = coordinate_grievances['x1']\n",
    "    coordinate_right = page.width\n",
    "    \n",
    "    if row['pages'] == 9:\n",
    "        coordinate_top = coordinate_grievances['top'] - 5\n",
    "        coordinate_bottom = coordinate_grievances['top'] + 30\n",
    "        \n",
    "    else:\n",
    "        coordinate_left = coordinate_left + 100\n",
    "        coordinate_top = coordinate_grievances['top'] - 5\n",
    "        coordinate_bottom = coordinate_grievances['top'] + 30\n",
    "\n",
    "    crop_coordinates = (coordinate_left, coordinate_top, coordinate_right, coordinate_bottom)\n",
    "        \n",
    "        \n",
    "    page = page.within_bbox(crop_coordinates)\n",
    "    \n",
    "    if page_count == 9:\n",
    "        return pd.DataFrame(\n",
    "            page.extract_table({\n",
    "                'snap_tolerance': 8\n",
    "            }),\n",
    "            columns = [\n",
    "                'ice',\n",
    "                'non_ice',\n",
    "                'total'\n",
    "            ]\n",
    "        ).astype(int).assign(\n",
    "            total=lambda x: x['ice'] + x['non_ice'],\n",
    "            file=row['file_name']\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return pd.DataFrame(\n",
    "            page.extract_table(),\n",
    "            columns=[\n",
    "                'q1',\n",
    "                'q2',\n",
    "                'q3',\n",
    "                'q4'\n",
    "            ]\n",
    "        ).replace('N/A', 0).astype(int).assign(\n",
    "            total=lambda x: x.sum(axis=1),\n",
    "            file=row['file_name']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = []\n",
    "\n",
    "for index, row in pdf_meta.iterrows():\n",
    "    payload.append(process_row(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(payload)[['file', 'total']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "pdfplumber helped collect data from the reports, but we also used [Overview](https://blog.overviewdocs.com/) for text search and OCR of non-machine-readable documents.\n",
    "\n",
    "Look at all the PDFs we parsed! But processing actually invovled quite a few things. We:\n",
    "\n",
    "- created pandas data frames about our data\n",
    "- wrote `for` loops\n",
    "- used regular expressions\n",
    "- filtered lists\n",
    "- used trial-and-error to find the perfect PDF coordinates\n",
    "- debugged errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
